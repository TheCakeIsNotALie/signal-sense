{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'signal_sense' from '/mnt/44785CD2785CC478/Users/yanni/Desktop/HEPIA/Year4/projet_semestre_terzina/signal-sense/signal_sense.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Avoid Tensorflow yelling\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from copy import deepcopy\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from pe_extractor.cnn import generator_nsb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import tensorflow as tf\n",
    "%matplotlib widget\n",
    "\n",
    "import importlib\n",
    "import signal_sense\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "importlib.reload(signal_sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sampling period of 5e-09s\n",
      "Having 10 bins for every sample\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generator variables\n",
    "n_sample = 100000\n",
    "n_sample_init = 0\n",
    "batch_size = 1\n",
    "shift_proba_bin = 30\n",
    "sigma_smooth_pe_ns = 0 #2.\n",
    "bin_size_ns = 0.5   \n",
    "sampling_rate_mhz = 200 #200 MHz is the sampling rate of terzina\n",
    "                        #250 SST1M\n",
    "                        #1000 LST + MST\n",
    "                        #rÃ©solution 100 picosecondes \n",
    "pe_rate_mhz = 150 # 1 to 150 MHz\n",
    "noise_lsb=1.5 # 0.5, 1, 1.5 options\n",
    "amplitude_gain=5.\n",
    "relative_gain_std=0.05\n",
    "# -------------------------------\n",
    "# Generator data\n",
    "# -------------------------------\n",
    "sampling_period_s = 1 / (sampling_rate_mhz * 1e6)\n",
    "bin_per_sample = ceil(((sampling_period_s) * 1e9) / bin_size_ns)\n",
    "\n",
    "gen = generator_nsb(\n",
    "    n_event=None, batch_size=batch_size, n_sample=n_sample + n_sample_init,\n",
    "    n_sample_init=n_sample_init, pe_rate_mhz=pe_rate_mhz,\n",
    "    bin_size_ns=bin_size_ns, sampling_rate_mhz=sampling_rate_mhz,\n",
    "    amplitude_gain=amplitude_gain, noise_lsb=noise_lsb,\n",
    "    sigma_smooth_pe_ns=sigma_smooth_pe_ns, baseline=0,\n",
    "    relative_gain_std=relative_gain_std, shift_proba_bin=shift_proba_bin, dtype=np.float64\n",
    ")\n",
    "\n",
    "print(f\"Using sampling period of {sampling_period_s}s\")\n",
    "print(f\"Having {bin_per_sample} bins for every sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ns_scale(start, size, resolution = 1):\n",
    "    return np.linspace((sampling_period_s * 1e9) * start, (sampling_period_s * 1e9) * (start+size), num=size * resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data :  (100000,)\n",
      "Generated outputs :  (100000,)\n",
      "Generated weights :  (100000,)\n",
      "Sliced : data=(99980, 21), results=(99980, 21), weights=(99980,)\n",
      "Train : data=(80000, 21), results=(80000, 21), weights=(80000,)\n",
      "Validation : data=(19980, 21), results=(19980, 21), weights=(19980,)\n"
     ]
    }
   ],
   "source": [
    "# CNN variables\n",
    "train_size = int(0.8 * n_sample)\n",
    "window_size = 21\n",
    "conv_size = 11\n",
    "\n",
    "data = next(gen)\n",
    "summed_bins = np.sum(data[1][0].reshape(-1, bin_per_sample), axis=1)\n",
    "print(\"Generated data : \", data[0][0].shape)\n",
    "print(\"Generated outputs : \", summed_bins.shape)\n",
    "# summed_bins_bool = list(map(lambda x: 1 if x > 0 else 0, summed_bins))\n",
    "\n",
    "\n",
    "sample_weights = signal_sense.compute_sample_weights(summed_bins)\n",
    "print(\"Generated weights : \", sample_weights.shape)\n",
    "\n",
    "sliced_data = sliding_window_view(data[0][0], window_size)\n",
    "sliced_results = sliding_window_view(summed_bins, window_size)\n",
    "sliced_weights = np.average(sliding_window_view(sample_weights, window_size), axis=1)\n",
    "print(f\"Sliced : data={sliced_data.shape}, results={sliced_results.shape}, weights={sliced_weights.shape}\")\n",
    "# print(sliced_weights)\n",
    "\n",
    "# split the data into training/test data\n",
    "dataset = list(zip(deepcopy(sliced_data), deepcopy(sliced_results), deepcopy(sliced_weights)))\n",
    "# np.random.shuffle(dataset)\n",
    "\n",
    "train_dataset, val_dataset = dataset[:train_size], dataset[train_size:]\n",
    "np.random.shuffle(train_dataset)\n",
    "train_data, train_results, train_weights = zip(*train_dataset)\n",
    "train_data, train_results, train_weights = np.asarray(train_data), np.asarray(train_results), np.asarray(train_weights)\n",
    "val_data, val_results, val_weights = zip(*val_dataset)\n",
    "val_data, val_results, val_weights = np.asarray(val_data), np.asarray(val_results), np.asarray(val_weights)\n",
    "print(f\"Train : data={train_data.shape}, results={train_results.shape}, weights={train_weights.shape}\")\n",
    "print(f\"Validation : data={val_data.shape}, results={val_results.shape}, weights={val_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (Batch  (None, 21, 1)             4         \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 11, 5)             60        \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 5, 5)              0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 5, 5)              0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 21)                546       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 610 (2.38 KB)\n",
      "Trainable params: 608 (2.38 KB)\n",
      "Non-trainable params: 2 (8.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.InputLayer(input_shape=(window_size, 1)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv1D(5, (conv_size)),\n",
    "        tf.keras.layers.MaxPooling1D(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(window_size, activation='relu'),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Tensor(\"Sum_2:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Sum_3:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Sum_2:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Sum_3:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node cond/IteratorGetNext/_5 defined at (most recent call last):\n<stack traces unavailable>\nThe second input must be a scalar, but it has shape [32,21]\n\t [[{{node cond/IteratorGetNext/_5}}]] [Op:__inference_train_function_1206]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m adam_optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39madam_optimizer,\n\u001b[1;32m      4\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;66;03m#   metrics=[signal_sense.f1_m, signal_sense.precision_m, signal_sense.recall_m])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[signal_sense\u001b[38;5;241m.\u001b[39mprecision_m])\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_results\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node cond/IteratorGetNext/_5 defined at (most recent call last):\n<stack traces unavailable>\nThe second input must be a scalar, but it has shape [32,21]\n\t [[{{node cond/IteratorGetNext/_5}}]] [Op:__inference_train_function_1206]"
     ]
    }
   ],
   "source": [
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer=adam_optimizer,\n",
    "              loss='mse',\n",
    "            #   metrics=[signal_sense.f1_m, signal_sense.precision_m, signal_sense.recall_m])\n",
    "              metrics=[signal_sense.precision_m])\n",
    "\n",
    "model.fit(train_data, train_results, epochs=10, validation_data=(val_data, val_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------------\n",
    "# # Moving window display NanoSecond scale\n",
    "# # --------------------------------\n",
    "# i = 0\n",
    "# resultsYMax = np.max(test_results) + 0.1\n",
    "# fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# def update():\n",
    "#     plt.clf()\n",
    "#     ax1 = plt.subplot(211)\n",
    "#     shape = val_data[0].shape[0]\n",
    "#     # print(shape, i)\n",
    "#     # print(shape * i, shape * (i+1))\n",
    "#     # print(\"Results shape : \" + str(test_results[i].shape))\n",
    "#     # print(\"Scale shape : \" + str(ns_scale(shape * i, shape, bin_resolution_multiplier).shape))\n",
    "#     # print(\"Model shape : \" + str(model.predict(val_data[i][None,], verbose=0)[0].shape))\n",
    "#     ax1.plot(ns_scale(shape * i, shape), val_data[i], label='Sensor data')\n",
    "#     ax1.set_ylabel('Amplitude')\n",
    "#     ax2 = plt.subplot(212, sharex=ax1)\n",
    "#     ax2.set_ylim(-0.1,resultsYMax)\n",
    "#     ax2.plot(ns_scale(shape * i, shape, bin_resolution_multiplier), test_results[i], label='Truth')\n",
    "#     # [None, ] is for the batch array dimension, since it's a single event, just add none\n",
    "#     ax2.plot(ns_scale(shape * i, shape, bin_resolution_multiplier), model.predict(val_data[i][None,], verbose=0)[0], '--r', label='Prediction')\n",
    "#     plt.xlabel('Sample index')\n",
    "#     plt.draw()\n",
    "\n",
    "# def press(event):\n",
    "#     global i, ax1, ax2\n",
    "#     # Move window left or right\n",
    "#     if event.key == 'left':\n",
    "#         i-=1\n",
    "#     elif event.key == 'right':\n",
    "#         i+=1\n",
    "#     update()\n",
    "\n",
    "# fig.canvas.mpl_connect('key_press_event', press)\n",
    "# update()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Moving window display\n",
    "# --------------------------------\n",
    "i = 0\n",
    "resultsYMax = np.max(test_results) + 0.1\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "def update():\n",
    "    plt.clf()\n",
    "    ax1 = plt.subplot(211)\n",
    "    ax1.plot(val_data[i], label='Sensor data')\n",
    "    ax1.set_ylabel('Amplitude')\n",
    "    ax2 = plt.subplot(212, sharex=ax1)\n",
    "    ax2.set_ylim(-0.1,resultsYMax)\n",
    "    ax2.set_ylabel('Number of pe events')\n",
    "    ax2.plot(test_results[i], label='Truth')\n",
    "    # [None, ] is for the batch array dimension, since it's a single event, just add none\n",
    "    ax2.plot(model.predict(val_data[i][None,], verbose=0)[0], '--r', label='Prediction')\n",
    "    plt.xlabel('Sample index')\n",
    "    plt.draw()\n",
    "\n",
    "def press(event):\n",
    "    global i, ax1, ax2\n",
    "    # Move window left or right\n",
    "    if event.key == 'left':\n",
    "        i-=1\n",
    "    elif event.key == 'right':\n",
    "        i+=1\n",
    "    update()\n",
    "\n",
    "fig.canvas.mpl_connect('key_press_event', press)\n",
    "update()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Whole waveform display\n",
    "# --------------------------------\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "fig.suptitle('Waveform PE event prediction')\n",
    "ax1 = plt.subplot(211)\n",
    "ax1.plot(data[0][0], label='Sensor data')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.legend()\n",
    "ax2 = plt.subplot(212, sharex=ax1)\n",
    "ax2.set_ylabel('Number of pe events')\n",
    "ax2.plot(summed_bins, label='Truth')\n",
    "results = model.predict(sliced_data)\n",
    "avg_results = signal_sense.reaggregate_windows_avg(results, window_size, window_size-1)\n",
    "ax2.plot(avg_results, label='Prediction')\n",
    "ax2.legend()\n",
    "plt.xlabel('Sample index')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_result = signal_sense.run_tests(model, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_pe_rate = [{ \"label\": f\"{k}Mhz\", \"data\" : list(g)} for k, g in itertools.groupby(metrics_result, key=operator.itemgetter('pe_rate_mhz'))]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "fig.suptitle('Model\\'s mean absolute error by configuration')\n",
    "\n",
    "x = np.arange(len(grouped_by_pe_rate))\n",
    "bar_width = 0.1\n",
    "multiplier = 0\n",
    "\n",
    "ax = fig.subplots(1)\n",
    "for i, entry in enumerate(grouped_by_pe_rate):\n",
    "    bar_label = [x[\"noise_lsb\"] for x in entry[\"data\"]]\n",
    "    bar_values = np.array([x[\"mean_absolute_error\"] for x in entry[\"data\"]]).flatten()\n",
    "    bar_locations = i + np.arange(len(bar_values)) * (bar_width + 0.01)\n",
    "    \n",
    "    rects = ax.bar(bar_locations, bar_values, bar_width, label=bar_label)\n",
    "    ax.bar_label(rects, rotation = 90, padding=3)\n",
    "    multiplier+=1\n",
    "\n",
    "ax.set_xticks(x, [x[\"label\"] for x in grouped_by_pe_rate])\n",
    "# ax.legend(loc='upper left', ncols=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('pe_extractor_multi.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('pe_extractor_multi.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_aware_model = quantize_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
